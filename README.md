# Proposta de Arquitetura para Web Scraping do G1 com Padr칚o Chain of Responsibility (Cadeia de reponsabilidade)


## 1. Objetivo do Projeto

- Criar um pipeline de dados f치cil de realizar manuten칞칚o, ou seja, estruturado, modular e escal치vel.  
- Discutir o padr칚o de projeto **Cadeia de Responsabilidade (Chain of Responsibility)**.  
- Gerar um arquivo `.docx` para cada not칤cia do G1.  
- Rastrear eventos de log em um banco de dados:  
  - Ex: **Sucesso de conex칚o da API**, **erro de conex칚o da API**, **consultar a not칤cia na API**, **salvar a not칤cia na API**.  
- Gravar a not칤cia em uma **API Simulada**.  

---

## 2. Arquitetura / Estrutura T칠cnica

### 2.1 Tecnologias Usadas
- **Linguagem:** Python  
- **Extra칞칚o:** BeautifulSoup  
- **Banco de dados:** SQLite para registro do log  
- **API Simulada:** para inserir as not칤cias  

---

## 3. Arquitetura do Pipeline

### 3.1 Diagrama de Classe
[![Diagrama de Classe](https://github.com/rodrigorocha1/web_scraping_g1/blob/master/diagramas/diagrama_de_classe.jpg?raw=true)](https://github.com/rodrigorocha1/web_scraping_g1/blob/master/diagramas/diagrama_de_classe.jpg?raw=true)

---

O padr칚o **Cadeia de Responsabilidade (Chain of Responsibility)** permite criar um fluxo composto por m칰ltiplos manipuladores, onde cada manipulador 칠 uma etapa de processamento de web scraping.  

As etapas listadas s칚o:
- **Checagem de conex칚o da API**  
- **Obter XML do site RSS do G1**  
- **Extrair link do conte칰do da not칤cia**  
- **Verificar se a not칤cia j치 existe**  
- **Salvar a not칤cia (API simulada e gerar .docx)**  

**Benef칤cios:**
- Alta coes칚o e baixo acoplamento  
- Extensibilidade: f치cil adicionar novas etapas sem alterar a l칩gica principal  
- Reuso: fluxo reaproveit치vel para outros sites de not칤cias  

---

### 3.2 Diagrama de Atividade
[![Diagrama de Classe](https://github.com/rodrigorocha1/web_scraping_g1/blob/master/diagramas/diagrama_de_atividade.jpg?raw=true)](https://github.com/rodrigorocha1/web_scraping_g1/blob/master/diagramas/diagrama_de_atividade.jpg?raw=true)

O diagrama mostra como ser치 o processo do web scraping:  
1. Conex칚o do site RSS: caso a conex칚o seja feita com sucesso, continua o web scraping; caso contr치rio, encerra o processo.  
2. Conex칚o do RSS + Parseamento (converter para formato leg칤vel).  
3. Conex칚o da URL do G1 presente no feed RSS.  
4. Verifica칞칚o se a not칤cia est치 cadastrada:  
   - **Se existe:** n칚o salva na API e encerra.  
   - **Se n칚o existe:** cadastra uma nova not칤cia e encerra o fluxo.  

---

## 4. Principais Funcionalidades

### Requisitos Funcionais
- Implementar busca pela URL.  
- Verificar se a not칤cia est치 cadastrada; caso contr치rio, cadastrar.  
- Validar texto da not칤cia.  
- Salvar a not칤cia com os campos:  
  - **id_noticia**, **t칤tulo**, **subt칤tulo**, **texto**, **autor**, **data_hora**.  
- Implementar tratamento de erros em todas as etapas.  

---

## 5. Requisitos N칚o Funcionais
- O c칩digo deve seguir os princ칤pios **SOLID**.  
- Utilizar o padr칚o **Cadeia de Responsabilidade**.  
- C칩digo com tipagem est치tica e documenta칞칚o.  
- O ETL n칚o deve precisar de refatora칞칚o extensa caso novos sites RSS sejam adicionados.  
- Suporte extens칤vel a outros sites RSS para extra칞칚o.  
- Todas as credenciais de acesso devem ser protegidas.  

---

## 6. Exemplo Simplificado do C칩digo

```python

from abc import ABC, abstractmethod
from typing import Optional

from src.context.pipeline_context import PipelineContext
from src.utils.db_handler import DBHandler
import logging

FORMATO = '%(asctime)s %(filename)s %(funcName)s %(module)s  - %(message)s'
db_handler = DBHandler(nome_pacote='Handler', formato_log=FORMATO, debug=logging.DEBUG)

logger = db_handler.loger


class Handler(ABC):

    def __init__(self) -> None:
        self._next_handler: Optional['Handler'] = None

    def set_next(self, hander: "Handler") -> "Handler":
        """
        Metodo para executar a cadeia
        :param hander: o Tipo de cadia
        :type hander: Handler
        :return: A Cadeia Ex: Conectar na api, acessar site
        :rtype: Handler
        """
        self._next_handler = hander
        return hander

    def handle(self, context: PipelineContext) -> None:
        """
        M칠todo que vai representar o fluxo do etl
        :param context: Recebe os contexto do pipeline
        :type context: PipelineContext
        :return: Nada
        :rtype: None
        """
        logger.info(f'{self.__class__.__name__} -> Iniciando web scraping')
        if self.executar_processo(context):
            logger.info(f'{self.__class__.__name__} -> Sucesso ao executar')
            if self._next_handler:
                self._next_handler.handle(context)
            else:
                logger.info(f'{self.__class__.__name__} ->  칔ltimo handler da cadeia')
        else:
            logger.warning(f'{self.__class__.__name__} -> Falha, pipeline interrompido')

    @abstractmethod
    def executar_processo(self, context: PipelineContext) -> bool:
        """
        M칠todo que vai representar o processo,ex: =Checar conex칚o na api
        :param context: contexto do pipeline, v치riaveis que se칚o passadas
        :type context: PipelineContext
        :return: Verdadeiro se o processo for executado com sucesso Falso caso contr치rio
        :rtype: bool
        """
        pass




from src.context.pipeline_context import PipelineContext
from src.handler_cadeia_pipeline.obternoticiag1handler import ObterUrlG1Handler
from src.handler_cadeia_pipeline.obterrsshandler import ObterRSSHandler
from src.handler_cadeia_pipeline.processar_noticia_handler import ProcessarNoticiaHandler
from src.handler_cadeia_pipeline.verificar_noticiag1_cadastadra_handler import VerificarNoticiaCadastradaHandler
from src.servicos.extracao.webscrapingbs4g1rss import WebScrapingBs4G1Rss
from src.servicos.extracao.webscrapingsiteg1 import WebScrapingG1
from src.servicos.manipulador.arquivo_docx import ArquivoDOCX
from src.servicos.s_api.noticia_api import NoticiaAPI
from src.handler_cadeia_pipeline.checarconexaohandler import ChecarConexaoHandler
from bs4 import BeautifulSoup
from typing import Generator, Dict, Any
from src.models.noticia import Noticia

rss_service = WebScrapingBs4G1Rss(url="https://g1.globo.com/rss/g1/sp/ribeirao-preto-franca")
g1_service = WebScrapingG1(url=None, parse="html.parser")
arquivo = ArquivoDOCX()
noticia_api = NoticiaAPI()

contexto = PipelineContext[Generator[Dict[str, Any], None, None]]()

p1 = ChecarConexaoHandler(api_noticia=noticia_api)

p2 = ObterRSSHandler[BeautifulSoup, Generator[Dict[str, Any], None, None]](
    servico_webscraping=rss_service
)
p3 = ObterUrlG1Handler[BeautifulSoup, Noticia](
    web_scraping_g1=g1_service
)
p4 = VerificarNoticiaCadastradaHandler(
    api_noticia=noticia_api
)

p5 = ProcessarNoticiaHandler(
    api_noticia=NoticiaAPI(),
    arquivo=ArquivoDOCX()
)

p1.set_next(p2) \
    .set_next(p3) \
    .set_next(p4) \
    .set_next(p5)



p1.handle(contexto)

```




## 7. V칤deo com a demonstra칞칚o do projeto 
 [![Assistir ao v칤deo de demonstra칞칚o do projeto](https://img.shields.io/badge/游꿟%20Assistir%20ao%20v칤deo-FF0000?style=for-the-badge&logo=youtube&logoColor=white)](https://youtu.be/S-rt9kp7MdY)

<div style="text-align:center;"> 



[Link do repos칤t칩rio](https://github.com/rodrigorocha1/web_scraping_g1)
